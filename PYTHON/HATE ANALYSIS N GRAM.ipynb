{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195e2718-3cba-4911-8eba-4f8de5f0b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\durga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\durga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\durga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name    Profile ID               Date  \\\n",
      "0         scotty2jatty  3.982741e+08  22/04/24 05:27:52   \n",
      "1     angelasanders975  6.596329e+10  22/04/24 17:43:49   \n",
      "2        myleslewis_24  1.168729e+10  22/04/24 17:49:54   \n",
      "3  matthew_williams224  2.524021e+10  22/04/24 18:53:42   \n",
      "4      vremyatherapper  3.178600e+09  22/04/24 19:21:58   \n",
      "\n",
      "                                Comment  Comment_Length  \\\n",
      "0                 better jakepaul dirty            34.0   \n",
      "1               thats soooooooooo right            24.0   \n",
      "2                        bro want shirt            21.0   \n",
      "3  michaelgalt bro needs beat jakes ass            60.0   \n",
      "4                              mike tko            28.0   \n",
      "\n",
      "                          Tokenized_Comment  \n",
      "0                 [better, jakepaul, dirti]  \n",
      "1                [that, soooooooooo, right]  \n",
      "2                        [bro, want, shirt]  \n",
      "3  [michaelgalt, bro, need, beat, jake, as]  \n",
      "4                               [mike, tko]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv(\"C:/Users/durga/Desktop/SPU/DS 600/PROJECT/FINAL/cleaned_data.csv\")\n",
    "\n",
    "# Tokenization and Preprocessing function\n",
    "def tokenize_and_preprocess(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stopwords.words('english') and word.lower() not in string.punctuation]\n",
    "    # Lemmatize and Stem\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization and preprocessing to the 'Comment' column\n",
    "df['Tokenized_Comment'] = df['Comment'].apply(tokenize_and_preprocess)\n",
    "\n",
    "# Display the preprocessed DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7caae6d6-dc70-4d78-81fc-390e1c647535",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = pd.read_csv(\"C:/Users/durga/Desktop/SPU/DS 600/PROJECT/FINAL/DATA FILES/HATE.csv\",encoding='latin1')\n",
    "hate['word'] = hate['word'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696926d7-c67a-4ed0-90bc-f5e9ccb4e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name    Profile ID               Date  \\\n",
      "0         scotty2jatty  3.982741e+08  22/04/24 05:27:52   \n",
      "1     angelasanders975  6.596329e+10  22/04/24 17:43:49   \n",
      "2        myleslewis_24  1.168729e+10  22/04/24 17:49:54   \n",
      "3  matthew_williams224  2.524021e+10  22/04/24 18:53:42   \n",
      "4      vremyatherapper  3.178600e+09  22/04/24 19:21:58   \n",
      "\n",
      "                                Comment  Comment_Length  \\\n",
      "0                 better jakepaul dirty            34.0   \n",
      "1               thats soooooooooo right            24.0   \n",
      "2                        bro want shirt            21.0   \n",
      "3  michaelgalt bro needs beat jakes ass            60.0   \n",
      "4                              mike tko            28.0   \n",
      "\n",
      "                          Tokenized_Comment   sentiment  \n",
      "0                 [better, jakepaul, dirti]   [Neutral]  \n",
      "1                [that, soooooooooo, right]   [Neutral]  \n",
      "2                        [bro, want, shirt]   [Neutral]  \n",
      "3  [michaelgalt, bro, need, beat, jake, as]  [negative]  \n",
      "4                               [mike, tko]   [Neutral]  \n"
     ]
    }
   ],
   "source": [
    "def assign_sentiment(tokenized_comment):\n",
    "    sentiments = []\n",
    "    for token in tokenized_comment:\n",
    "        # Check if token exists in hate dataset\n",
    "        if token in hate['word'].values:\n",
    "            sentiment = hate.loc[hate['word'] == token, 'sentiment'].iloc[0]\n",
    "            sentiments.append(sentiment)\n",
    "    # If no sentiment found, assume neutral\n",
    "    if not sentiments:\n",
    "        sentiments.append('Neutral')\n",
    "    return sentiments\n",
    "\n",
    "# Apply tokenization and preprocessing to the 'Comment' column\n",
    "df['Tokenized_Comment'] = df['Comment'].apply(tokenize_and_preprocess)\n",
    "\n",
    "# Assign sentiments to tokenized comments\n",
    "df['sentiment'] = df['Tokenized_Comment'].apply(assign_sentiment)\n",
    "\n",
    "# Display the preprocessed DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1b578f-790a-4c3f-a38c-a356fc422ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for offensive topic:\n",
      "['like', 'one', 'lol', 'get', 'even', 'presid', 'need', 'biden', 'come', 'know']\n",
      "\n",
      "Top words for non-offensive topic:\n",
      "['trump', 'biden', 'woman', 'like', 'vote', 'go', 'get', 'presid', 'peopl', 'one']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to join tokenized words into strings\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Joining tokenized words into strings\n",
    "df['Tokenized_Comment_String'] = df['Tokenized_Comment'].apply(join_tokens)\n",
    "\n",
    "# Creating CountVectorizer to convert tokenized comments into a bag of words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Tokenized_Comment_String'])\n",
    "\n",
    "# Training LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=2, random_state=42)  \n",
    "lda_model.fit(X)\n",
    "\n",
    "# Getting the topic-word matrix\n",
    "topic_word_matrix = lda_model.components_\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Define functions to get top words for each topic\n",
    "def get_top_words(topic_index, n_top_words):\n",
    "    return [feature_names[i] for i in topic_word_matrix[topic_index].argsort()[:-n_top_words - 1:-1]]\n",
    "\n",
    "# Get top words for each topic\n",
    "n_top_words = 10  \n",
    "top_words_offensive = get_top_words(0, n_top_words)\n",
    "top_words_non_offensive = get_top_words(1, n_top_words)\n",
    "\n",
    "print(\"Top words for offensive topic:\")\n",
    "print(top_words_offensive)\n",
    "print(\"\\nTop words for non-offensive topic:\")\n",
    "print(top_words_non_offensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc547b7a-cc86-4706-b7aa-b2bab0da438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Join tokens into a single string\n",
    "    text = ' '.join(text)\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    # Removing punctuation and non-alphanumeric characters\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fdf5916-d66b-404f-bed6-f931095513d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(inferred_topics, model):\n",
    "    # Example: If the sum of topic weights for offensive topics is greater than non-offensive, predict offensive sentiment\n",
    "    offensive_topics_sum = inferred_topics[:, 0].sum()\n",
    "    non_offensive_topics_sum = inferred_topics[:, 1].sum()\n",
    "\n",
    "    if offensive_topics_sum > non_offensive_topics_sum:\n",
    "        return 'Offensive'\n",
    "    else:\n",
    "        return 'Non-offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68ed344d-8611-4e24-9cbe-b56051f8455b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define and train the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      7\u001b[0m new_comment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour new comment dumb ass here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m preprocessed_new_comment \u001b[38;5;241m=\u001b[39m preprocess_text(new_comment)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define and train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "new_comment = \"Your new comment dumb ass here\"\n",
    "preprocessed_new_comment = preprocess_text(new_comment)\n",
    "\n",
    "# Convert the preprocessed comment into a bag of words representation\n",
    "new_comment_vectorized = vectorizer.transform([preprocessed_new_comment])\n",
    "\n",
    "# Use the trained LDA model to infer topics\n",
    "new_comment_topics = lda_model.transform(new_comment_vectorized)\n",
    "\n",
    "# Assuming you have a function to predict sentiment using the inferred topics and the trained model\n",
    "predicted_sentiment = predict_sentiment(new_comment_topics, model)\n",
    "\n",
    "print(\"Predicted sentiment for the new comment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924303d4-0ca9-4386-a7d2-0b0f44e75324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
